# -*- coding: utf-8 -*-
"""Logistic Reg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1emLmuDJ5_LlQ0fvzVVOL6fL5fcaniaz-
"""

import numpy as np

import matplotlib.pyplot as plt

from math import e

from math import log

class logistic_regression(object):

  def sigmoid(self,X):                                                          #definition of the sigmoid function
    s = 1/(1+e**(-X))
    return s                                                                    #returns the sigmoid(x)        

  def fit(self,x_train,y_train,k):                                              #training the model using default values of no. of iterations,learning rate,without regularisation
    self.k = k                                                                  #k is the no. of classes
    x = np.array(x_train)
    m = x.shape[0]
    n = x.shape[1]
    X = np.zeros((m,n))
    Y = np.zeros((m,self.k))                                                          
    for a in range(m):                                                          #converting y into one Vs all type matrix
      Y[a,y_train[a]]=1
    for a in range(n):                                                          #normalising the data
            X[:,a] = (x[:,a]-np.mean(x[:,a]))/(np.std(x[:,a])+1e-09)
    self.theta = np.zeros((self.k,n+1))                                         #initialising theta matrix(paramaters)
    X = np.hstack((np.ones((m,1)),X))
    H = np.dot(X,self.theta.T)                                                   
    G = self.sigmoid(H)                                                         #computing the hypothesis function(G)
    cost_func = -np.sum(Y*np.log(G)+(1-Y)*np.log(1-G))/m                        #defining the cost function
    alpha = 0.1                                                                 #setting a default learning rate of 0.1
    self.l = []
    self.p = []
    for c in range(3000):                                                       #performing gradient descent step for a default value of 3000 iterations
      self.p.append(c)
      H = np.dot(X,self.theta.T)
      G = self.sigmoid(H)
      cost_func = -np.sum(Y*np.log(G)+(1-Y)*np.log(1-G))/m
      self.l.append(cost_func)
      self.theta = self.theta - (alpha/m)*(np.dot((G-Y).T,X))                   #modifying the parameters matrix(theta) using the partial derivative of the cost function with respect to the thetas

  def train(self,x_train,y_train,iter,alpha,lamda,k):                           #training the model using input values of iterations,learning rate and regularisation parameter
    self.k = k
    x = np.array(x_train)
    m = x.shape[0]
    n = x.shape[1]
    X = np.zeros((m,n))
    Y = np.zeros((m,self.k))
    for a in range(m):                                                          #converting y into one Vs all type matrix
      Y[a,y_train[a]]=1
    for a in range(n):                                                          #normalising the data
            X[:,a] = (x[:,a]-np.mean(x[:,a]))/(np.std(x[:,a])+1e-09)            
    self.theta = np.zeros((self.k,n+1))                                         #initialising theta matrix
    X = np.hstack((np.ones((m,1)),X))
    H = np.dot(X,self.theta.T)
    G = self.sigmoid(H)                                                         #hypothesis function
    cost_func = -np.sum(Y*np.log(G)+(1-Y)*np.log(1-G))/m + (lamda/(2*m))*np.sum(self.theta[:,1:]**2)     #cost function considering regularisation 
    for a in range(iter):                                                       #performing gradient descent over input given iterations
      H = np.dot(X,self.theta.T)
      G = self.sigmoid(H)
      cost_func = -np.sum(Y*np.log(G)+(1-Y)*np.log(1-G))/m + (lamda/(2*m))*np.sum(self.theta[:,1:]**2)
      theta1 = np.zeros((self.k,n+1))
      theta1[:,1:] = self.theta[:,1:]                                               
      self.theta = self.theta - (alpha/m)*(np.dot((G-Y).T,X) + lamda*theta1)    #updating theta using gradient of theta and also regularistaion parameter

  def get_params(self):                                                         #returns the weights(theta)
    return self.theta

  def predict(self,x_test):                                                     #predicting the value of y using the trained model
    x = np.array(x_test)
    m = x.shape[0]
    n = x.shape[1]
    X = np.zeros((m,n))
    for a in range(n):                                                          #normalising the data
            X[:,a] = (x[:,a]-np.mean(x[:,a]))/(np.std(x[:,a])+1e-09)
    X = np.hstack((np.ones((m,1)),X))
    H = np.dot(X,self.theta.T)
    G = self.sigmoid(H)                                                         #hypothesis(G)
    prediction = G.argmax(axis = 1).reshape((m,1))                              #predicting the value of y based on the maximum probable class
    return prediction

  def accuracy(self,y_pred,y_true):                                             
      p = 0
      m = len(y_true)
      for a in range(m):                                                        #classifies a prediction as correct only if it is equal to the original class
        if y_true[a]==y_pred[a]:                
          p+=1
      return (p*100)/m

  def split_train_test(self,X,y,split):                                         #splits the data into training set and testing set based on input split fraction
        m = size()
        n = split*np.size(y,axis=0)
        x_train = X[0:n,:]
        x_test = X[n:,:]
        y_train = y[0:n]
        y_test = y[n:]
        return x_train,x_test,y_train,y_test

  def plot(self):                                                               #plots the cost function Vs the no. of iterations
      plt.plot(self.p,self.l)