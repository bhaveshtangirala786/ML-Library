# -*- coding: utf-8 -*-
"""Linear_reg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IqSSDmuXn_q93NbpPimJ4JTZkbs7lIRQ
"""

import numpy as np

import matplotlib.pyplot as plt

from math import e

class linear_regression(object):

    def fit(self,x_train,y_train):                                              #training the model using default values of no. of iterations,learning rate,without regularisation
        x = np.array(x_train)
        m = np.size(x,axis = 0)
        n = np.size(x,axis = 1)
        X = np.zeros((m,n))
        for a in range(n):                                                      #normalising the data
            X[:,a] = (x[:,a]-np.mean(x[:,a]))/(np.std(x[:,a])+1e-09)
        X = np.hstack((np.ones((m,1)),X))
        y = np.array(y_train).reshape(m,1)   
        self.theta = np.zeros((n+1,1))                                          #initialsing theta matrix
        hypothesis = np.dot(X,self.theta)                                       #computing the hypothesis function
        cost_func = (((hypothesis-y)**2).sum())/(m*2)                           #defining the cost function
        alpha = 0.0001                                                          #setting a default learning rate of 0.0001
        self.iter_count = 0
        self.l=[]
        self.p=[]
        while self.iter_count<7000:                                             #performing gradient descent step for a default value of 7000 iterations
            self.theta = self.theta - (alpha/m)*(np.dot(X.T,(hypothesis-y)))    #updating the values of the theta matrix
            hypothesis = np.dot(X,self.theta)
            cost_func = (((hypothesis-y)**2).sum())/(m*2)
            self.l.append(cost_func)
            self.p.append(self.iter_count)
            self.iter_count+=1

    def train(self,x_train,y_train,iter,alpha,lamda):                           #training the model using input values of iterations,learning rate and regularisation parameter
        x = np.array(x_train)
        m = np.size(x,axis = 0)
        n = np.size(x,axis = 1)
        X = np.zeros((m,n))
        for a in range(n):                                                      #normalising the data
            X[:,a] = (x[:,a]-np.mean(x[:,a]))/(np.std(x[:,a])+1e-09)
        X = np.hstack((np.ones((m,1)),X))
        y = np.array(y_train).reshape(m,1)   
        self.theta = np.zeros((n+1,1))                                          #initialising theta matrix
        hypothesis = np.dot(X,self.theta)                                       #computing the hypothesis function
        cost_func = (((hypothesis-y)**2).sum()+np.sum(self.theta[1:]**2)*lamda)/(m*2)   #cost function considering regularisation            
        for a in range(iter):                                                   #performing gradient descent over input given iterations
          theta1 = np.zeros((n+1,1))
          theta1[1:] = self.theta[1:]
          hypothesis = np.dot(X,self.theta)
          cost_func = (((hypothesis-y)**2).sum()+np.sum(self.theta[1:]**2)*lamda)/(m*2)
          self.theta = self.theta - (alpha/m)*(np.dot(X.T,(hypothesis-y))-lamda*theta1)  #updating theta using gradient of theta and also regularistaion parameter
 
          
    def get_params(self):                                                       #returns the weights(theta)
        return self.theta

    def predict(self,x_test):                                                   #predicting the value of y using the trained model
        x = np.array(x_test)
        m = np.size(x,axis = 0)
        n = np.size(x,axis = 1)
        X = np.zeros((m,n))
        for a in range(n):                                                      #normalising the data
            X[:,a] = (x[:,a]-np.mean(x[:,a]))/(np.std(x[:,a])+1e-09)   
        X = np.hstack((np.ones((m,1)),X))
        prediction = np.dot(X,self.theta)                                       #predicting the output using the theta matrix
        return prediction

    def score(self,y_pred,y_test):                                              #calculating the score of the prediction made by the model
        self.y_test = np.array(y_test)
        u = ((self.y_test.reshape((y_test.shape[0],1))-y_pred**2)).sum()        
        v = np.sum((y_test-self.y_test.mean())**2)
        return 1-u/v

    def split_train_test(self,X,y,split):                                       #splits the data into training set and testing set based on input split fraction
        m = size()
        n = split*np.size(y,axis=0)
        x_train = X[0:n,:]
        x_test = X[n:,:]
        y_train = y[0:n]
        y_test = y[n:]
        return x_train,x_test,y_train,y_test

    def plot(self):                                                             #plots the cost function Vs the no. of iterations
        plt.plot(self.p,self.l)